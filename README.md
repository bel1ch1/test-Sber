Консольный чат-бот (RAG) на базе локальной LLM
==============================================


Содержание
----------

- [Преимущества системы](#преимущества-системы)
- [Требования](#требования)
- [Структура проекта](#структура-проекта)
- [Как использовать приложение](#как-использовать-приложение)
  - [Главное меню](#главное-меню)
  - [1) Чат с ассистентом](#1-чат-с-ассистентом)
  - [2) Добавить документы в систему](#2-добавить-документы-в-систему)
  - [3) Удалить все документы из системы](#3-удалить-все-документы-из-системы)
  - [Оценка качества RAG (RAGAS) через `golden_set`](#оценка-качества-rag-ragas-через-golden_set)
  - [Подключение OpenAI для оценки](#подключение-openai-для-оценки)
- [Запуск через Docker Compose](#запуск-через-docker-compose)
  - [Установка NVIDIA Container Toolkit (только для GPU)](#установка-nvidia-container-toolkit-только-для-gpu)
  - [Старт системы (модели скачиваются автоматически)](#старт-системы-модели-скачиваются-автоматически)
  - [Запуск консольного приложения](#запуск-консольного-приложения)
  - [Запуск оценки RAG](#запуск-оценки-rag)
  - [Остановка и очистка](#остановка-и-очистка)

Преимущества системы
--------------------

- **RAG по своим документам**: можно загрузить тексты и получать ответы на основе своей базы знаний.
- **Потоковая индексация**: документы индексируются чанками и батчами без загрузки всего набора в память.
- **Встроенная оценка на `golden_set`**: воспроизводимая проверка качества на эталонных вопросах/ответах.
- **Оценка “лжи”/галлюцинаций**: в метриках есть отдельная проверка на негативных примерах (`is_negative`), где правильный ответ - строго `Информация недоступна`.
- **Агентный подход с планированием**: решения о retrieval/tools принимаются моделью на этапе маршрутизации.
- **Защита от prompt injection**: системный промпт включает жесткие правила и границы.

Требования
----------

- **Docker Engine** и **Docker Compose v2**
- **NVIDIA GPU + драйвер** (если нужен GPU-режим Ollama)
- **NVIDIA Container Toolkit** на хосте

Структура проекта
-----------------

- `main.py`: консольное меню и пользовательский сценарий (чат, загрузка/удаление документов).
- `chains/`: агентная логика диалога.
- `prompts/`: сборка промптов и системные правила.
- `rag/`: ретривер.
- `tools/`: инструменты, которые ассистент может вызывать.
- `llm_wrapper/`: инициализация LLM.
- `evaluate_rag.py`: оценка RAG.
- `rag_eval_config.json`: конфиг тестирования RAG.
- `utils/`: вспомогательные модули.
- `scripts/`: вспомогательные скрипты.
- `data/uploads/`: папка для документов и артефактов оценки.
- `Dockerfile`: образ приложения (Python + зависимости).
- `docker-compose.yml`: поднимает Qdrant + Ollama, скачивает модели, запускает приложение/оценку.

Как использовать приложение
---------------------------

Ниже описаны все способы взаимодействия с программой, которые реализованы в `main.py`.

### Главное меню

После запуска вы увидите меню:

- **1) Чат с ассистентом**
- **2) Добавить документы в систему**
- **3) Удалить все документы из системы**
- **4) Выход**

Нужно ввести номер пункта и нажать Enter.

### 1) Чат с ассистентом

В этом режиме вы общаетесь с ассистентом. Внутри одного сеанса ведётся история диалога.

- **Ввод**: печатайте сообщение в строке `Вы:`.
- **Выход из чата в меню**: введите `exit` или `quit`.
- **RAG**: включен (ассистент при необходимости может получаить контекст из Qdrant).
- **Tools**: включены (ассистент при необходимости может вызывать встроенные инструменты).

### 2) Добавить документы в систему

Режим для индексации документов в Qdrant.

1. Положите файлы в папку `data/uploads`.
2. В главном меню выберите пункт `2`.
3. Введите команду:
   - `upload` — начать индексацию
   - `back` — вернуться в меню

Важно про форматы:
- Сейчас индексируются **только `.txt`** (файлы с другими расширениями будут пропущены и выведены в списке).

### 3) Удалить все документы из системы

Полностью очищает коллекцию Qdrant (файлы в `data/uploads` не удаляются).

1. В главном меню выберите пункт `3`.
2. Подтвердите удаление, введя `yes`, `y` или `да`.
3. Любой другой ввод отменит операцию.

### Оценка качества RAG (RAGAS) через `golden_set`

Оценка запускается отдельной командой (через Docker Compose, см. ниже) и использует:

- **`data/uploads/golden_set.json`** - список тестов, каждый элемент вида:
  - `input`: вопрос
  - `expected_output`: эталонный ответ (строка)
  - `source_file`: ожидаемый источник (имя файла или список файлов)
  - `is_negative` (опционально): если `true`, правильный ответ - строго `Информация недоступна`

Конфиг оценки - `rag_eval_config.json`:

- **`sample_ratio`**: доля выборки из `golden_set` (1.0 = весь набор)
- **`sample_seed`**: seed для воспроизводимости
- **`top_k`**: сколько документов извлекать из ретривера
- **`generator_model`**: модель, которая генерирует ответы (Ollama)
- **`judge_provider`**: `ollama` или `openai`
- **`judge_model`**: модель судьи (например, `mistral:7b` или `gpt-4o-mini` при указанном api_key)
- **`qdrant_collection`**: имя коллекции
- **`qdrant_embed_model`**: embedding-модель в Ollama (`nomic-embed-text`)
- **`ragas_timeout_s`**, **`ragas_max_retries`**, **`ragas_max_workers`**: параметры устойчивости/параллелизма RAGAS

Результаты сохраняются в `data/uploads/rag_eval_results.json` (summary + per-sample).

### Подключение OpenAI для оценки

Чтобы использовать OpenAI как `judge_provider`, сделайте:

1. В `rag_eval_config.json`:
   - `judge_provider`: `openai`
   - `judge_model`: нужная модель (например, `gpt-4o-mini`)
2. Перед запуском оценки передайте ключ в окружение:

```bash
export OPENAI_API_KEY="..."
```

Опционально (если нужно):
- `OPENAI_BASE_URL` — свой endpoint
- `OPENAI_ORG` — организация

Запуск через Docker Compose
---------------------------

Проект запускается через **Docker Compose**: поднимаются Qdrant + Ollama (GPU) и автоматически скачиваются нужные модели.

### Установка NVIDIA Container Toolkit (только для GPU)

Если toolkit не установлен, используйте официальную инструкцию NVIDIA (добавляет репозиторий)
из документации `https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html`:

```bash
sudo apt-get update && sudo apt-get install -y --no-install-recommends curl gnupg2 ca-certificates
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
  | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
  | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

Проверка, что GPU виден внутри контейнера:

```bash
docker run --rm --gpus all nvidia/cuda:12.8.0-base-ubuntu24.04 nvidia-smi
```

### Старт системы (модели скачиваются автоматически)

В корне проекта:

```bash
docker compose up -d
```

Что произойдет:
- Поднимутся `qdrant` и `ollama`.
- Сервис `ollama-bootstrap` дождется готовности Ollama и скачает модели: `qwen2.5:7b`, `mistral:7b`, `nomic-embed-text`.

Посмотреть прогресс скачивания моделей:

```bash
docker compose logs -f ollama-bootstrap
```

Если вы хотите изменить список скачиваемых моделей, отредактируйте переменную `OLLAMA_PULL_MODELS` в `docker-compose.yml`.

### Запуск консольного приложения

Приложение интерактивное, поэтому запускается через профиль `cli`:

```bash
docker compose --profile cli run --rm app
```

### Запуск оценки RAG

Перед запуском убедитесь, что вы проиндексировали документы (через меню приложения).

Запуск оценки:

```bash
docker compose --profile eval run --rm rag-eval
```

Если используете OpenAI как judge, передайте ключ:

```bash
OPENAI_API_KEY="..." docker compose --profile eval run --rm rag-eval
```

### Остановка и очистка

Остановка контейнеров:

```bash
docker compose down
```

Полная очистка (включая данные Qdrant и скачанные модели Ollama):

```bash
docker compose down -v
```

Логирование
-----------

Логи ассистента пишутся в отдельную корневую папку `logs/` в формате JSON (файл `logs/agent.log`).
Папка создается автоматически при первом событии логирования.

Что логируется:
- маршрутизация запроса (решение: retrieval/tools/none);
- результаты retrieval (сколько чанков найдено);
- планирование и выбор инструмента;
- ошибки вызова инструментов;
- финализация ответа (длина ответа);
- завершение каждого хода диалога (ввод, ответ, решение, инструмент).

Когда это происходит:
- при запуске приложения включается логгер;
- на каждом шаге графа агента и по завершению каждого диалога.
